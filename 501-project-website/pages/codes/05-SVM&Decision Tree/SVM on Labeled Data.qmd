## Using SVM to classify text data with labels.

Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.

In this document, I use SVM to classify the cleaned text data with labels. The dataset is from Twitter with the keyword personal credit, associated with my record data, whose labels are the result of sentiment analysis, which are neutral, negative and positive.

To improve the accuracy of the classifier, I also perform parameter selection to improve the performance of the model by reducing the number of parameters and eliminating some parameters that are not highly relevant. Next, I debug the parameters of the model through a series of methods to find the most suitable hyperparameters for the model, and train the optimal model to arrive at the final classifier.

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

text = pd.read_csv("Labeled_text_data.csv")
text = text.drop(columns=['Unnamed: 0'])
text.head()
```

**Distributions of class labels**

```{python}
text.sentiment.value_counts()
```

## Baseline Model of SVM

Before text classification by SVM, first you have to split the words, create the bow model, and then vectorize the words with countvectorizer() function in scikit-learn.

```{python}
text['token'] = text['text'].str.split(' ')
text['token'] = text['token'].astype(str)
text.head()

from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer(max_features=10000)
BOW = vectorizer.fit_transform(text['token'])
```

Split the data into training and test sets and view the dimensions.

```{python}
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
X = BOW
y = text['sentiment']
x_train,x_test,y_train,y_test = train_test_split(BOW,np.asarray(text["sentiment"]))

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)
```

Create the svm classification model, set the initial kernel to rbf, train the data, and record the time spent training the model. Then make predictions.

```{python}
from sklearn.svm import SVC
import time

start_time = time.time()

model = SVC(kernel='rbf')
model.fit(x_train,y_train)

end_time = time.time()
process_time = round(end_time-start_time,2)
print("Fitting SVC took {} seconds".format(process_time))

yp_train = model.predict(x_train)
yp_test = model.predict(x_test)
```

**Calculate the accuracy on both train set and test set.** 

The result on test set is much more higher than that on train set.

```{python}
from sklearn.metrics import accuracy_score,confusion_matrix

print("Accuracy(train) of model is {}%".format(accuracy_score(y_test,yp_test) * 100))
print("Accuracy(test) of model is {}%".format(accuracy_score(y_train,yp_train) * 100))
```

**Confusion Matrix** 

confusion matrix of training set

```{python}
cm = confusion_matrix(y_train, yp_train, labels=model.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
disp.plot()
plt.show()
```

confusion matrix of test set

```{python}
cm = confusion_matrix(y_test, yp_test, labels=model.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
disp.plot()
plt.show()
```

**Then calculate the precision and recall for each set.**

```{python}
from sklearn.metrics import classification_report
# train
print("------TRAIN------")
print(classification_report(y_train,yp_train))
# test
print("------TEST------")
print(classification_report(y_test,yp_test))
```

## Feature Selection

In this part, I used the univariate feature selection. Univariate feature selection works by selecting the best features based on univariate statistical tests. It can be seen as a preprocessing step to an estimator.

SelectKBest removes all but the  highest scoring features

We can see the effect of this model where the shape of X changes from (250,828) to (250,10). k=10 is the value of manual settings.

```{python}
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
X_new = SelectKBest(chi2, k=10).fit_transform(X, y)
print(X.shape)
print(X_new.shape)
```

## Model Tuning

In this part, I used GridSearchCV() method to tune the best hyperparameters for the SVM model. Parameters to be tuned are C, gamma and kernel.

```{python}
from sklearn.model_selection import GridSearchCV
param_grid = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'sigmoid']}
grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=2)
grid.fit(x_train,y_train)
print(grid.best_estimator_)
```

See the classification report and confusion matrix of the optimal model.

```{python}
from sklearn.metrics import classification_report, confusion_matrix

grid_predictions = grid.predict(x_test)
print(confusion_matrix(y_test,grid_predictions))
print(classification_report(y_test,grid_predictions))
print(accuracy_score(y_test,grid_predictions))
```

## Result Summary

In the whole process of modeling, the model perfomance was obviously promoted.

Although the prediction accuracy of the tuned model did not improve on the test set, it found the most suitable hyperparameters for this dataset.

In addition, after feature selection, the vectors generated by the bow model are dimensionally reduced, which is a good way to overcome the drawback of sparse bow model.