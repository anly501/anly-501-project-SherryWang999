---
title: "data gather by R"
format: html
editor: visual
---

## Introduction

This data gathering process was finished by python and used Twitter API to grab some text data from recent tweets. Then I painted a wordcloud to show which words in the topic appear most. It is easy but will be useful to my future work to finish the project.

I'm gonna use the NLP methods to do some sentiment analysis and the result will be discussed in my project portfolio.

## Theory

**Twitter api**

The Twitter API is a set of programmatic endpoints that can be used to understand or build the conversation on Twitter. This API allows you to find and retrieve, engage with, or create a variety of different resources including the following: Tweets. Users.

**Wordcloud**

Word clouds or tag clouds are graphical representations of word frequency that give greater prominence to words that appear more frequently in a source text. The larger the word in the visual the more common the word was in the document(s).

```{r}
library("selectr")
library("rvest")
library("xml2")
library(rtweet) # for scraping tweets
library(wordcloud2) # for generating really cool looking wordclouds
library(tm) # for text minning
library(dplyr) # loads of fun stuff including piping
library(twitteR)
library(ROAuth)
library(jsonlite)
```

```{r}
consumerKey = 'Apx8TT3mHth1uKedUxcHPiR2Q'
consumerSecret = 'A6vKdRrqHvz9EDL5WJQJMzXwlH7fCUMV8MbQn2ULkwM5XXrVqk'
access_Token = '1298010161949421568-UJZh6bAJOv6EJl13VP1cp2wolnPguG'
access_Secret = 'lwsZju1YqcNbv8B6ETgVGbq6MWn8zNZGpfUfPM6aAwxVr'
```

```{r}
### Properly parse our API keys 

consumerKey=as.character(consumerKey)
consumerSecret=as.character(consumerSecret)
access_Token=as.character(access_Token)
access_Secret=as.character(access_Secret)

requestURL='https://api.twitter.com/oauth/request_token'
accessURL='https://api.twitter.com/oauth/access_token'
authURL='https://api.twitter.com/oauth/authorize'
```

```{r}
setup_twitter_oauth(consumerKey,consumerSecret,access_Token,access_Secret)
```

```{r}
Search1<-twitteR::searchTwitter("Personal Credit",n=500, since="2022-04-01")
#Search2<-twitteR::searchTwitter("#photography",n=50, since="2022-01-01")
TweetsDF<- twListToDF(Search1)
#(TweetsDF$text[1])

########## Place Tweets in a new file ###################
FName = "MyFileExample.txt"
## Start the file
MyFile <- file(FName)
## Write Tweets to file
cat(unlist(TweetsDF), " ", file=MyFile, sep="\n")
close(MyFile)
```

```{r}
TweetsDF = twListToDF(Search1)

TweetsDF$text <- gsub("[^[:alnum:][:blank:]?&/\\-]", "",TweetsDF$text) # remove alphanumeric characters 
TweetsDF$text <- gsub("https\\S*", "",TweetsDF$text) # remove hyperlinks
#TweetsDF$text = gsub("(?!(#|@))[[:punct:]]", "", text, perl = T) 
# remove all punctuations except # and @. 
```

```{r}
#create a corpus to allow us clean the text column with tm
tweets.corpus <- Corpus(VectorSource(TweetsDF$text))


tweets.corpus <- tweets.corpus %>%
  tm_map(removeNumbers) %>% # removes numbers from text
  tm_map(removePunctuation) %>% # removes punctuation from text
  tm_map(stripWhitespace) %>% # trims the text of whitespace
  tm_map(content_transformer(tolower)) %>% # convert text to lowercase
  tm_map(removeWords,stopwords("english")) %>% # remove stopwords
  tm_map(removeWords,stopwords("SMART")) # remove stopwords not removed from previous line
tdm <- TermDocumentMatrix(tweets.corpus) %>% # create a term document matrix
      as.matrix()

words <- sort(rowSums(tdm), decreasing = TRUE) # count all occurrences of each word and group them
df <- data.frame(word = names(words), freq = words) # convert it to a dataframe
head(df) # visualize!
```

```{r}
wcloud <- wordcloud2(df,   # generate word cloud
                     size = 1.5,
                     color= 'random-dark', # set colors
                     #shape = 'pentagon',
                     rotateRatio = 0) #horizontal looks better, but what do you think?
wcloud
```

## Conclusions

Text cleaning is task-specific and one needs to have a strong idea about what they want their end result to be and even review the data to see what exactly they can achieve.

Here I summarized some steps to clean text data:

\*\*Most common methods for Cleaning the Data\*\*

\* Lowecasing the data

\* Removing Puncuatations

\* Removing Numbers

\* Removing extra space

\* Replacing the repetitions of punctations

\* Removing Emojis

\* Removing emoticons

\* Removing Contractions

But we should keep in mind that there is actually no fixed way to clean a data set. All we need to do is to find a suitable method to clean your own data set.
